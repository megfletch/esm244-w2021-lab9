---
title: 'Lab 9: Bootstrapping & Nonlinear Least Squares'
author: "Meghan Fletcher"
date: "3/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(here)
library(boot)
library(gt)
library(patchwork)
library(broom)
library(nlstools)
```

## Part 1: Fun, beautiful tables with 'gt'

Simplify the data a bit to get the 5 countries with the lowest savings ratio:
```{r}
disp_income <- LifeCycleSavings %>% 
  rownames_to_column() %>% 
  arrange(dpi) %>% 
  head(5) %>% 
  mutate(ddpi = ddpi/100,
         pop15 = pop15/100,
         pop75 = pop75/100) # To make it a decimal
```

Make a table using 'gt' package:
- Percent variables (ddpi, pop15 and pop75) should be in percent format
- Per capita disposable income (dpi) should be as dollars
- Color of dpi cells should change based on value

```{r}
disp_income %>% 
  gt() %>% 
  tab_header(
    title = "Life cycle savings",
    subtitle = "5 countries with lowest per capita disposable income"
  ) %>% 
  fmt_currency(
    columns = vars(dpi),
    decimals = 2
  ) %>% 
  fmt_percent(
    columns = vars(pop15, pop75, ddpi),
    decimals = 1
  ) %>% 
  fmt_number(
    columns = vars(sr),
    decimals = 1
  ) %>% 
  tab_options(
    table.width = pct(80) # Update table width
  ) %>% 
  tab_footnote( # Add a footnote
    footnote = "Data averaged from 1970 - 1980",
    location = cells_title()
  ) %>% 
  data_color( # Update cell colors...
    columns = vars(dpi),
    colors = scales::col_numeric(
      palette = c(
        "orange", "red", "purple"), # Overboard colors! 
      domain = c(120, 190) # Scale endpoints (outside will be gray)
      )
    ) %>% 
  cols_label(
    sr = "Savings ratio",
    pop15 = "Pop < 15yr",
    pop75 = "Pop <75yr",
    dpi = "disposable $ percapita",
    ddpi = "Disposable percent"
  )
```

## Part 2: Bootstrapping

Bootstrap a 95% confidence interval for the mean salinity of river discharge in Pamlico Sound, NC (see `?salinity` for information on the dataset, which exists in the `boot`) package. 

```{r}
# Get some summary statistics from the single salinity sample:
hist(salinity$sal)
mean(salinity$sal)
t.test(salinity$sal) # Get 95% CI for t-distribution
```

### Bootstrap the mean salinity:

Bootstrap the mean salinity by first creating a function to calculate the mean for each of our bootstrap samples

```{r}
# First, create a function that will calculate the mean of each bootstrapped sample
mean_fun <- function (x,i) {mean(x[i])}

# Then, get just the vector of salinity (salinity$sal)
sal_nc <- salinity$sal

# Now, create 100 bootstrap samples by resampling from the salinity vector (sal_nc), using the function you created (mean_fun) to calculate the mean of each:
salboot_100 <- boot(sal_nc,
                    statistic = mean_fun,
                    R = 100)

# OK, then for comparison, let's also create 10000 bootstrap samples with replacement
salboot_10k <- boot(sal_nc,
                    statistic = mean_fun,
                    R = 10000)

# Check out the output from the bootstrap:
salboot_100
salboot_10k

# Question: do we all get the same thing? What would we have to do in order to all get the same thing (hint: set.seed())? And would we *want* that? 
```

```{r}
# Use $t0 element from the `boot` output to see the original sample mean, and $t to see the means for each of the bootstrap samples:
salboot_100$t0 # The original sample mean
salboot_100$t # These are all the means for each of the 100 bootstrap samples

# Make vectors of bootstrap sample means a data frame (so ggplot will deal with it). 
salboot_100_df <- data.frame(bs_mean = salboot_100$t)
salboot_10k_df <- data.frame(bs_mean = salboot_10k$t)

# ggplot the bootstrap sample medians: 

# The histogram of the original sample:
p1 <- ggplot(data = salinity, aes(x =sal)) +
  geom_histogram()

# Histogram of 100 bootstrap sample means:
p2 <- ggplot(data = salboot_100_df, aes(x = bs_mean)) +
  geom_histogram()

# Histogram of 10k bootstrap sample means:
p3 <- ggplot(data = salboot_10k_df, aes(x = bs_mean)) +
  geom_histogram()

# Aside: remember that {patchwork} is awesome. 
(p1 + p2 + p3) & theme_minimal()
```

Use `boot.ci()` to find the confidence interval for the bootstrapped distribution (here, with the 10k bootstrapped means):

```{r}
boot.ci(salboot_10k, conf = 0.95)
```

A reminder on what a confidence interval means: For a 95% confidence interval, that means we expect that 95 of 100 calculated confidence intervals will contain the actual population parameter. 

What it does **not** mean: There's a 95% chance that your confidence interval contains the true population parameter (it either does or it doesn't)...

## Part 3: Nonlinear least squares

Nonlinear least squares (NLS) finds parameter estimates to minimize the sum of squares of residuals, using an iterative algorithm. We'll use NLS to find the parameters for a logistic growth equation. 

Read in some mock logistic growth data:

```{r}
df <- read_csv(here("data","log_growth.csv"))

ggplot(data = df, aes(x = time, y = pop)) +
  geom_point() +
  theme_minimal() +
  labs(x = "time (hr)", y = "population (ind)")

# log transformed data
ggplot(data = df, aes(x = time, y = log(pop))) +
  geom_point() +
  theme_minimal() +
  labs(x = "time (hr)", y = "ln(population)")
```

Recall the logistic growth equation: 

$P(t)=\frac{K}{1+Ae^{-kt}}$, where

- $K$ is the carrying capacity
- $A$ is $\frac{K-P_0}{P_0}$
- $k$ is the growth rate constant

Estimate the growth constant during exponential phase (to get a starting-point guess for *k*): 

```{r}
# Get only up to 14 hours & log transform the population
# We do this so we can estimate the growth rate constant (k) *during exponential growth phase)
df_exp <- df %>% 
  filter(time < 15) %>% 
  mutate(ln_pop= log(pop))

# Model linear to get *k* estimate (the slope of this linear equation is an estimate of the growth rate constant):
lm_k <- lm(ln_pop ~ time, data = df_exp)
lm_k
```

Now we have initial estimate for *k* (0.17), and we can estimate *K* ~180 and *A* ~ 17. We need those estimates because we will use them as starting points for interative algorithms trying to converge on the parameters. If we're too far off, they may not converge or could converge on the very wrong thing.

We'll estimate the parameters using nonlinear least squares (NLS): 

```{r}
df_nls <- nls(pop ~ K/(1 + A*exp(-r*time)),
              data = df,
              start = list(K = 180, A = 17, r = 0.17),
              trace = TRUE)

# Note: you can add argument `trace = TRUE` to see the different estimates at each iteration (and the left-most column reported tells you SSE overall)

# See the model summary (null hypothesis: parameter value = 0)
summary(df_nls)

# Use broom:: functions to get model outputs in tidier format
model_out <- broom::tidy(df_nls) 

# Want to just get one of these?  
A_est <- model_out$estimate[2] # Gets the first row in the estimate column.
```

Our model with estimated parameters is:
$$P(t) = \frac{188.7}{1+138.86e^{-0.35t}}$$

### Visualize model over original observed values

```{r}
# Make predictions for the population at all of those times (time) in the original df:
p_predict <- predict(df_nls)

# Bind predictions to original data frame:
df_complete <- data.frame(df, p_predict)

# plot them all together
ggplot(data = df_complete, aes(x = time, y = pop)) +
  geom_point() +
  geom_line(aes(x = time, y = p_predict)) +
  theme_minimal()
```

### Find confidence intervals for parameter estimates

See `?confint2` and `?confint.nls` (in `nlstools` package)

```{r}
df_ci <- confint2(df_nls)
df_ci
```



